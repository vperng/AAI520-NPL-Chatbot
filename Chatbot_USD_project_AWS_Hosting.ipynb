{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a406fc75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de29ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bdbe4c",
   "metadata": {},
   "source": [
    "# Connect to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c0e25859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available S3 buckets:\n",
      "sagemaker-studio-443370720429-v31y0qov1w\n",
      "sagemaker-us-east-2-443370720429\n",
      "usd-projects\n",
      "s3://usd-projects/final_model-20241009T224405Z-001.zip\n"
     ]
    }
   ],
   "source": [
    "# Create an S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List all S3 buckets\n",
    "response = s3.list_buckets()\n",
    "\n",
    "# Print bucket names\n",
    "print(\"Available S3 buckets:\")\n",
    "for bucket in response['Buckets']:\n",
    "    print(bucket['Name'])\n",
    "bucket_name = bucket['Name']\n",
    "\n",
    "# List the objects in the bucket\n",
    "response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "# Print the full path of the zip file\n",
    "for obj in response.get('Contents', []):\n",
    "    if obj['Key'].endswith('.zip'):\n",
    "        print(f\"s3://{bucket_name}/{obj['Key']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "c6730e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of the bucket 'usd-projects':\n",
      "final_model-20241009T224405Z-001.tar.gz\n",
      "final_model-20241009T224405Z-001.zip\n",
      "inference.py\n",
      "requirement.txt\n"
     ]
    }
   ],
   "source": [
    "# List objects in the bucket\n",
    "response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "\n",
    "# Check if the bucket is not empty\n",
    "if 'Contents' in response:\n",
    "    print(f\"Contents of the bucket '{bucket_name}':\")\n",
    "    for obj in response['Contents']:\n",
    "        print(obj['Key'])\n",
    "else:\n",
    "    print(f\"The bucket '{bucket_name}' is empty or does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2f820825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the bucket and file names\n",
    "bucket_name = 'usd-projects'\n",
    "file_name = 'final_model-20241009T224405Z-001.zip'\n",
    "download_path = '/home/ec2-user/SageMaker/final_model.zip'  # Path to download the zip file\n",
    "\n",
    "# Download the zip file\n",
    "s3.download_file(bucket_name, file_name, download_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7b1d4468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference.py  requirements.txt\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "ls /home/ec2-user/SageMaker/source_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "20824e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Specify the extraction path\n",
    "extraction_path = '/home/ec2-user/SageMaker/final_model'\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extraction_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ccb0f",
   "metadata": {},
   "source": [
    "# Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6f8226d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Set the paths to the model and tokenizer directories\n",
    "model_dir = os.path.join(extraction_path, '/home/ec2-user/SageMaker/final_model/final_model')  \n",
    "tokenizer_dir = os.path.join(extraction_path, '/home/ec2-user/SageMaker/final_model/final_model')  \n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "15af4063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Output:\n",
      " The Great Wall of China stretches over 13,000 miles and is a series of fortifications.  How long is the Great Wall of China?  13,000 miles\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define a test input (make sure it's compatible with your model, e.g., distilgpt2)\n",
    "test_input = \"<|context|> The Great Wall of China stretches over 13,000 miles and is a series of fortifications. <|question|> How long is the Great Wall of China?\"\n",
    "input_ids = tokenizer.encode(test_input, return_tensors='pt').to(model.device)\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Decode the output\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Output:\")\n",
    "print(decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5aede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(context, question):\n",
    "    # Combine context and question into the input text\n",
    "    input_text = f\"<|context|> {context} <|question|> {question}\"\n",
    "\n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(model.device)\n",
    "\n",
    "    # Create attention mask\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).type(torch.int).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate output from the model\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=150,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "    # Decode the output\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "\n",
    "    # Extract the answer from the decoded output\n",
    "    answer_start = decoded_output.find(\"<|answer|>\") + len(\"<|answer|>\")\n",
    "    if answer_start != -1:\n",
    "        answer = decoded_output[answer_start:].strip()  # Extract answer\n",
    "        # Remove any padding tokens from the end of the answer\n",
    "        answer = answer.split('[PAD]')[0].strip()  # Trim padding tokens\n",
    "    else:\n",
    "        answer = \"No answer token found.\"\n",
    "\n",
    "    print(f\"Decoded Output: {decoded_output}\")  # Keep this for debugging\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lists of contexts and questions\n",
    "contexts = [\n",
    "    \"The Eiffel Tower is located in Paris, France. It is one of the most recognizable structures in the world.\",\n",
    "    \"The Great Wall of China stretches over 13,000 miles and is a series of fortifications.\",\n",
    "    \"Mount Everest is the highest mountain in the world, with a peak that reaches 29,029 feet.\",\n",
    "    \"The Amazon Rainforest is home to an estimated 390 billion trees and millions of species of plants and animals.\",\n",
    "    \"The Mona Lisa, painted by Leonardo da Vinci, is one of the most famous works of art in history.\"\n",
    "]\n",
    "\n",
    "questions = [\n",
    "    \"Where is the Eiffel Tower located?\",\n",
    "    \"How long is the Great Wall of China?\",\n",
    "    \"What is the height of Mount Everest?\",\n",
    "    \"What is significant about the Amazon Rainforest?\",\n",
    "    \"Who painted the Mona Lisa?\"\n",
    "]\n",
    "\n",
    "# Loop through each context and question\n",
    "for context, question in zip(contexts, questions):\n",
    "    answer = generate_response(context, question)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print()  # Print a newline for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce9252f",
   "metadata": {},
   "source": [
    "# Deploy Model to Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "302422b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50261, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50261, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "84655a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = 'arn:aws:iam::443370720429:role/service-role/AmazonSageMaker-ExecutionRole-20241010T105606'  # Replace with your IAM role with SageMaker permissions\n",
    "\n",
    "# Define the S3 path for your model\n",
    "model_path = \"s3://usd-projects/final_model-20241009T224405Z-001.tar.gz\"\n",
    "\n",
    "# Create the PyTorch model object\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=model_path,\n",
    "    role=role,\n",
    "    entry_point=\"inference.py\",  # Your inference script\n",
    "    source_dir=\"source_dir\",  # Directory containing the inference script\n",
    "    framework_version='1.12',  # Match the PyTorch version you used\n",
    "    py_version='py38',  # Match the Python version\n",
    "    dependencies=[\"source_dir/requirements.txt\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e0d26582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "predictor = pytorch_model.deploy(\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    container_startup_health_check_timeout=600  # Increase timeout (in seconds)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b313093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input data\n",
    "test_input = \"<|context|> The Great Wall of China stretches over 13,000 miles and is a series of fortifications. <|question|> How long is the Great Wall of China?\"\n",
    "\n",
    "# Log the input to verify\n",
    "print(f\"Sending input: {test_input}\")\n",
    "\n",
    "response = predictor.predict(\n",
    "    test_input,\n",
    "    initial_args={\n",
    "        \"ContentType\": \"text/plain\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fa8f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_context1 = [\"Where is the Eiffel Tower located?\",\"The Eiffel Tower is located in Paris, France. It is one of the most recognizable structures in the world.\"]\n",
    "question_context2 = [\"How long is the wall of China?\", \"The Great Wall of China stretches over 13,000 miles and is a series of fortifications.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b6d6e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference:\n",
      "Question: \u001b[1mWhere is the Eiffel Tower located?\u001b[0m\n",
      "Context: The Eiffel Tower is located in Paris, France. It is one of the most recognizable structures in the world.\n",
      "model answer: \u001b[1mParis, France\u001b[0m\n",
      "\n",
      "Inference:\n",
      "Question: \u001b[1mHow long is the wall of China?\u001b[0m\n",
      "Context: The Great Wall of China stretches over 13,000 miles and is a series of fortifications.\n",
      "model answer: \u001b[1m13,000 miles\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newline = '\\n'\n",
    "bold = '\\033[1m'\n",
    "unbold = '\\033[0m'\n",
    "def query_endpoint(encoded_text):\n",
    "    endpoint_name = 'usd-chatbot-20241012-055920'\n",
    "    client = boto3.client('runtime.sagemaker')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, ContentType='application/list-text', Body=encoded_text)\n",
    "    return response\n",
    "\n",
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response['Body'].read())\n",
    "    answer = model_predictions['answer']\n",
    "    return answer\n",
    "\n",
    "for question_context in [question_context1, question_context2]:\n",
    "    query_response = query_endpoint(json.dumps(question_context).encode('utf-8'))\n",
    "    answer = parse_response(query_response)\n",
    "    print (f\"Inference:{newline}\"\n",
    "            f\"Question: {bold}{question_context[0]}{unbold}{newline}\"\n",
    "            f\"Context: {question_context[1]}{newline}\"\n",
    "            f\"model answer: {bold}{answer}{unbold}{newline}\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1376bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vperng/AAI520-NPL-Chatbot/blob/main/Chatbot_Project_Team6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJVt0v8fpRaG"
      },
      "source": [
        "# Advanced Generative Chatbot Design\n",
        "\n",
        "Rene Ortiz, Vivian Perng, Karthink Raghavan\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "- Goal: Build a chatbot that can carry out multi-turn conversations, adapt to context, and handle a variety of topics.\n",
        "- Output: A web or app interface where users can converse with the chatbot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/aai_env/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain_community.llms import Ollama\n",
        "from langchain_community.document_loaders import DataFrameLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.load import dumpd, dumps, load, loads\n",
        "from langchain.chains import load_chain\n",
        "from langserve import add_routes\n",
        "from langchain_core.runnables import RunnableBinding, RunnableLambda\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "import uvicorn\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import logging  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def squad1_json_to_dataframe(file_path, record_path=['data', 'paragraphs', 'qas', 'answers']):\n",
        "    \"\"\"\n",
        "    Functuon to convert the dataset JSON file to a Pandas DataFrame.\n",
        "\n",
        "    file_path (str): Path to the JSON file\n",
        "    record_path (list): Path to the deepest level in the JSON structure (default is ['data', 'paragraphs', 'qas', 'answers']).\n",
        "\n",
        "    Returns dataFrame containing the parsed data.\n",
        "    \"\"\"\n",
        "    # Load JSON data\n",
        "    with open(file_path, 'r') as f:\n",
        "        file_data = json.load(f)\n",
        "\n",
        "    # Extract and normalize the nested JSON structures\n",
        "    answers_df = pd.json_normalize(file_data, record_path)\n",
        "    questions_df = pd.json_normalize(file_data, record_path[:-1])\n",
        "    paragraphs_df = pd.json_normalize(file_data, record_path[:-2])\n",
        "\n",
        "    # Create 'context' by repeating the corresponding paragraph for each question\n",
        "    questions_df['context'] = np.repeat(paragraphs_df['context'].values, paragraphs_df.qas.str.len())\n",
        "    questions_df['answers'] = answers_df['text']\n",
        "\n",
        "    # Create final DataFrame with necessary columns\n",
        "    data = questions_df[['id', 'question', 'context', 'answers']].copy()\n",
        "\n",
        "    return data.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>Saint Bernadette Soubirous</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is in front of the Notre Dame Main Building?</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>a copper statue of Christ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>the Main Building</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the Grotto at Notre Dame?</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>a Marian place of prayer and reflection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What sits on top of the Main Building at Notre...</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>a golden statue of the Virgin Mary</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  To whom did the Virgin Mary allegedly appear i...   \n",
              "1  What is in front of the Notre Dame Main Building?   \n",
              "2  The Basilica of the Sacred heart at Notre Dame...   \n",
              "3                  What is the Grotto at Notre Dame?   \n",
              "4  What sits on top of the Main Building at Notre...   \n",
              "\n",
              "                                             context  \\\n",
              "0  Architecturally, the school has a Catholic cha...   \n",
              "1  Architecturally, the school has a Catholic cha...   \n",
              "2  Architecturally, the school has a Catholic cha...   \n",
              "3  Architecturally, the school has a Catholic cha...   \n",
              "4  Architecturally, the school has a Catholic cha...   \n",
              "\n",
              "                                   answers  \n",
              "0               Saint Bernadette Soubirous  \n",
              "1                a copper statue of Christ  \n",
              "2                        the Main Building  \n",
              "3  a Marian place of prayer and reflection  \n",
              "4       a golden statue of the Virgin Mary  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the SQuAD dataset\n",
        "file_path = \"train-v1.1.json\"\n",
        "df = squad1_json_to_dataframe(file_path, record_path=['data', 'paragraphs', 'qas', 'answers'])\n",
        "df = df.drop(columns=['id'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/0_/skr2d259613gs33jbnv_ytn80000gn/T/ipykernel_37835/181025031.py:29: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(\n"
          ]
        }
      ],
      "source": [
        "# Load data into Langchain\n",
        "loader = DataFrameLoader(df, page_content_column=\"context\")\n",
        "docs = loader.load()\n",
        "\n",
        "logging.info(\"split documents into chunks.\")\n",
        "# Split documents into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "documents = text_splitter.split_documents(docs)\n",
        "\n",
        "# Add metadata to the docs (like question and answer_text)\n",
        "for doc in docs:\n",
        "    doc.metadata[\"question\"] = doc.metadata.get(\"question\", \"Unknown\")\n",
        "    doc.metadata[\"answer_text\"] = doc.metadata.get(\"answer_text\", \"No answer\")\n",
        "\n",
        "# Filter out any documents with None in metadata\n",
        "docs = [doc for doc in docs if all(value is not None for value in doc.metadata.values())]\n",
        "\n",
        "logging.info(\"Creating embedding.\")\n",
        "\n",
        "\n",
        "# Initialize the SentenceTransformer model\n",
        "model_name = 'all-MiniLM-L6-v2'\n",
        "model_kwargs = {'device': device}\n",
        "encode_kwargs = {'normalize_embeddings': True}\n",
        "\n",
        "#sentence_transformer_model = SentenceTransformer(model_name).to(device)\n",
        "\n",
        "# Wrap the SentenceTransformer model with LangChain's HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding dimensions: 384\n"
          ]
        }
      ],
      "source": [
        "# Generate a sample embedding to check dimensions\n",
        "sample_text = \"This is a sample document to check embedding dimensions.\"\n",
        "sample_embedding = embeddings.embed_documents([sample_text])  # Use the embed method to generate an embedding\n",
        "\n",
        "# Print the dimensions of the embedding\n",
        "embedding_dimension = len(sample_embedding[0])  # Get the length of the first embedding\n",
        "print(f\"Embedding dimensions: {embedding_dimension}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added batch 1 to Chroma vector store.\n",
            "Added batch 2 to Chroma vector store.\n",
            "Added batch 3 to Chroma vector store.\n"
          ]
        }
      ],
      "source": [
        "logging.info(\"Loading to vector db\")\n",
        "\n",
        "# Load to Chroma vector store\n",
        "max_batch_size = 41666  # Maximum batch size allowed\n",
        "for i in range(0, len(documents), max_batch_size):\n",
        "    batch_docs = documents[i:i + max_batch_size]\n",
        "    # Add each batch to the Chroma vector store\n",
        "    db = Chroma.from_documents(documents=batch_docs, embedding=embeddings, persist_directory=\"./\", collection_name=\"squadembedding\")\n",
        "\n",
        "    print(f\"Added batch {i // max_batch_size + 1} to Chroma vector store.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "logging.info(\"initialize retriever\")\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "\n",
        "# Define the LLM and prompt template\n",
        "logging.info(\"initialize model and prompt.\")\n",
        "\n",
        "llm = Ollama(model=\"llama2\") \n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Answer the following question based only on the provided context. \n",
        "Think step by step before providing a detailed answer. \n",
        "<context>\n",
        "{context} \n",
        "</context>\n",
        "Question: {input}\"\"\")\n",
        "\n",
        "logging.info(prompt)\n",
        "\n",
        "logging.info(\"creating document chains.\")\n",
        "\n",
        "document_chain=create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "logging.info(\"creating retrieval chains.\")\n",
        "\n",
        "# Create the retrieval-based document chain\n",
        "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, one of the major differences between the Bosniaks, Croats, and Serbs is their geographical identification. The context highlights that Slavs often identify themselves with the local geographical region in which they live, and this is especially true for the Bosniaks, Croats, and Serbs.\\n\\nThe Bosniaks are identified as living in southern Bosnia, while the Croats are identified as living in westernmost Croatia. The Serbs are identified as descendants of the Grenzers who continued to live in the area known as the Military Frontier until the Croatian war of independence.\\n\\nTherefore, one of the major differences between these ethnic groups is their geographical identification and association with specific regions within the broader South Slavic region.'"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query =\"What was one of the major differences between the Bosniaks, Croats and Serbs?\"\n",
        "response = retrieval_chain.invoke({\"input\": query})\n",
        "response['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "logging.info(\"initialize retriever\")\n",
        "db1 = Chroma(persist_directory=\"./\", embedding_function=embeddings, collection_name=\"squadembedding\")\n",
        "\n",
        "retriever1 = db1.as_retriever()\n",
        "\n",
        "\n",
        "# Define the LLM and prompt template\n",
        "logging.info(\"initialize model and prompt.\")\n",
        "\n",
        "llm1 = Ollama(model=\"llama2\") \n",
        "\n",
        "\n",
        "\n",
        "prompt1 = ChatPromptTemplate.from_template(\"\"\"\n",
        "Answer the following question based only on the provided context. \n",
        "Think step by step before providing a detailed answer. \n",
        "<context>\n",
        "{context} \n",
        "</context>\n",
        "Question: {input}\"\"\")\n",
        "\n",
        "logging.info(prompt1)\n",
        "\n",
        "logging.info(\"creating document chains.\")\n",
        "\n",
        "document_chain1=create_stuff_documents_chain(llm1, prompt1)\n",
        "\n",
        "logging.info(\"creating retrieval chains.\")\n",
        "\n",
        "# Create the retrieval-based document chain\n",
        "retrieval_chain1 = create_retrieval_chain(retriever1, document_chain1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, the answer to the question \"What is the capital of India?\" is New Delhi.'"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query1 =\"what is the capital of india?\"\n",
        "response1 = retrieval_chain1.invoke({\"input\": query1})\n",
        "response1['answer']"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aai_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
